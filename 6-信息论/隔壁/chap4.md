--- 第 1 页 ---

**信源表示：率失真特性**

**4**

本讲和下一讲关注信息源发出的消息的表示，如第1讲图1.1所示。假设信息源发出的消息属于一个有限集合 S={a1,a2,...,a∣S∣}。 那么，如果希望用一串二进制数字，即比特，来无歧义地表示该消息，显然这串数字的长度至少应为 ⌈log2∣S∣⌉。 但我们能用更短的字符串来完成这个任务吗？ 如果我们允许表示中存在一些模糊性，答案是肯定的。 例如，考虑 S={1,2,3,4,5,6,7,8}，并假设希望再现消息的目的地不在乎再现的消息与源消息的差异是否超过1。 那么，我们可以安全地将1、2、3中的任何一个表示为2，将4、5、6中的任何一个表示为5，将7、8中的任何一个表示为8。因此，长度为 ⌈log23⌉=2 的二进制字符串就足够了，而不是在不允许任何模糊性时所需的 ⌈log28⌉=3。

从上面的简单例子中，我们清楚地看到，用于表示源消息的资源量（例如，二进制字符串的长度）与再现消息的质量（例如，模糊程度）之间存在一种权衡。 如果考虑到信源的概率性质，这种权衡将变得更加有趣。 率失真理论描述了这种权衡的基本极限，是本讲的主题。 它的极端情况，即失真为零或几乎为零的情况，需要特殊处理，并将在下一讲中进行研究。

4.1 问题描述 43 4.2 香农信源编码基本定理 46 4.3 逆定理部分的证明 53 4.4 可达性部分的证明 55 4.4.1 码本的生成 56 4.4.2 编码 56 4.4.3 期望失真分析 58 4.4.4 编码失败概率的估计 58 4.4.5 总结步骤 60

**4.1 问题描述**

在第1讲图1.1的一般通信系统模型中，我们将信息源建模为一个产生随机过程 S1,S2,... 的概率设备。一个消息则对应于该随机过程的一个指定长度 n 的段，即 S=[S1,S2,...,Sn]。

在本讲中，我们关注信息源是离散无记忆信源（DMS）的情况； 也就是说，S1,S2,... 是一系列独立同分布（i.i.d.）的随机变量，每个变量的概率质量函数（pmf）为 PS(s)，字母表为 S。

--- 第 2 页 ---

**44**

**4 信源表示：率失真特性**

为了表示一个信源消息，我们需要为 S 的每一个可能值 s∈Sn 分配一个从某个有限集合中选出的索引，不失一般性，这个集合可以固定为 {1,2,...,Mn}。 这个分配通过一个映射完成：

fn(s):Sn↦{1,2,...,Mn}, (4.1)

我们称之为**信源编码器**。 这里的下标 n 用来强调对信源消息长度的依赖性，上标 (s) 用来表示该映射用于信源编码，以区别于第6讲中的信道编码。给定 fn(s)，索引 W=fn(s)(S) 就是由信源消息随机向量 S 导出的一个随机变量。

假设索引 W 已被告知目的地。 目的地需要将信源消息再现为 S^=[S^1,S^2,...,S^n]，这也是一个长度为 n 的随机向量。 但我们允许 S^i, i=1,2,...,n 在一个字母表 S^ 中取值，这个字母表通常可能与信源字母表 S 不同。 这种再现通过一个映射完成：

gn(s):{1,2,...,Mn}↦S^n, (4.2)

我们称之为**信源译码器**。 总结来说，我们有以下马尔可夫链：

S↔W=fn(s)(S)↔S^=gn(s)(W)=gn(s)(fn(s)(S))。 (4.3)

图4.1展示了信源编码和译码过程。

*图4.1：信源编码和译码示意图。DMS（离散无记忆信源）产生 S，经过信源编码器得到 W，再经过信源译码器得到 Ŝ。*

**图4.1**：信源编码和译码示意图。

**注4.1** 这个问题的关键特征是马尔可夫链关系（4.3）。 实际上，可以用一些条件概率分布 PW∣S 和 PS^∣W 分别替代确定性映射 (fn(s) 和 gn(s))，而本讲的核心结果，即下一节的香农信源编码基本定理，仍然成立。

**注4.2** 允许再现字母表 S^ 与信源字母表 S 不同，初看起来可能有些奇怪。 但这大大增加了问题描述的适用性，因为它使得目的地能够完成与信源相关的不同任务。 例如，在图像分类任务中，信源是一个图像，而目的地感兴趣的是决定该图像属于哪个类别（例如，动物、人物或风景）。

--- 第 3 页 ---

在这个例子中，信源消息（即图像）是一个像素阵列，而再现的消息只是一个指示信源消息类别的标签。

一个信源编码器将每个长度为n的信源消息段表示为Mn个索引中的一个，这可以存储为一个长度为⌈log2Mn⌉的二进制字符串。 平均而言，每个信源符号因此由⌈log2Mn⌉/n个比特表示。 我们澄清一下，这里的术语“比特”对应于计算机等设备中的存储单位，不应与我们在第2讲中介绍的信息度量混淆。

因此，我们将信源编码器/译码器对的**速率**定义为：

R=n⌈log2Mn⌉ 比特/信源符号。 (4.4)

由于我们允许在目的地的再现消息中存在一定程度的模糊性，我们在这里引入**失真**的概念。 **失真度量** d 是一个映射，它为每对 (s,s^)∈S×S^ 分配一个非负数 d(s,s^)，用以量化将 s 再现为 s^ 所产生的成本。 在我们的讲义中，除非另有说明，我们考虑有界失真度量； 也就是说，dmax:=max(s,s^)∈S×S^d(s,s^)<∞。

**例 4.1** 对于**汉明失真**，我们有 S^=S，并设如果 s=s^ 则 d(s,s^)=0，否则为 1。 注意对于汉明失真，E[d(S,S^)]=P(S=S^).

对于一个信源编码器/译码器对 (fn(s),gn(s))，我们进一步对 S 和 S^ 之间的失真施加一个可加结构：

d(s,s^)=n1∑i=1nd(si,s^i); (4.5)

也就是说，一个信源消息与其对应的再现消息之间的失真是每个信源符号与其对应的再现符号之间成对失真的平均值。 对于例4.1中的汉明失真，d(s,s^) 则是再现消息 s^ 中“错误”的比例。

拥有一个低速率和小失真的信源编码器/译码器对无疑是理想的。 由马尔可夫链 S↔W↔S^ 导出，失真 d(S,S^) 是一个随机变量。 这一考虑导致了以下可达率失真对的定义。

**定义 4.1** 如果存在一个信源编码器/译码器对的序列，

**4.1 问题描述 45**

--- 第 4 页 ---

**46**

**4 信源表示：率失真特性**

*图 4.2：率失真域和 R(D) 函数示意图。图中横轴为失真 D，纵轴为速率 R。阴影区域为率失真域，其左上边界曲线为 R(D) 函数。*

**图4.2**：率失真域和 R(D) 函数示意图。

{(fn(s),gn(s))}n=1,2,..., 满足

R≥n⌈log2Mn⌉ 比特/信源符号， (4.6)

limn→∞E[d(S,S^)]≤D. (4.7)

则称一个率失真对 (R, D) 是**可达的**。

取 (R,D) 平面第一象限中可达率失真对集合的闭包*，我们得到**率失真域**。 因此，率失真域的边界引出了**率失真函数**及其逆函数**失真率函数**的概念。 见图 4.2 的示意图。

**定义 4.2** **率失真函数** R(D) 是对于每个给定的 D，使得 (R,D) 在率失真域中的速率的下确界； 而**失真率函数** D(R) 是对于每个给定的 R，使得 (R,D) 在率失真域中的失真的下确界。

值得注意的是，率失真域是在渐近意义上定义的：信源编码器/译码器对在消息大小 n→∞ 时对任意大的消息都有效。 采用渐近分析是信息论的一个关键，并且在渐近状态下常常会出现奇观。

**4.2 香农信源编码基本定理**

在本节中，我们将介绍一个基本结果，该结果首次出现在香农的文章[16]中，它描述了率失真函数。

- 对我们的目的而言，将一个集合的闭包理解为该集合与其边界的并集就足够了。

--- 第 5 页 ---

**4.2 香农信源编码基本定理 47**

让我们定义信息率失真函数如下。

**定义 4.3** 对于一个概率质量函数为 PS(s) 的离散无记忆信源 S 和失真度量 d(s,s^), (s,s^)∈S×S^，以下约束优化问题的解：

minPS^∣SI(S;S^), (4.8)

s.t. E[d(S,S^)]≤D (4.9)

被称为**信息率失真函数** RI(D)。

香农信源编码基本定理断言信息率失真函数等于率失真函数。

**定理 4.1** 对于一个概率质量函数为 PS(s) 的离散无记忆信源 S 和有界失真度量 d(s,s^), (s,s^)∈S×S^，我们有

R(D)=RI(D) (4.10)

请注意，定理4.1中的率失真函数不是以闭合形式给出的。 它涉及一个优化问题 (4.8)-(4.9)。目标函数是信源 S 与其再现 S^ 之间的互信息 I(S;S^)。由于 PS,S^(s,s^)=PS(s)PS^∣S(s^∣s) 且 PS 是给定的，最小化 I(S;S^) 是相对于 PS^∣S 进行的。 约束 E[d(S,S^)]≤D 对 PS^∣S 的可行选择施加了限制。

根据定义4.2，率失真函数 R(D) 具有编码解释，即在一个足够长的信源消息块上，受期望失真约束 D 的一系列编码器/译码器对所能达到的速率的下确界。 另一方面，由定义4.3给出的信息率失真函数 RI(D) 只是一个数学对象，一个依赖于 PS 和 d 的函数，没有任何编码解释。 是定理4.1将这两者联系起来，断言它们实际上是相等的。 有了定理4.1，当我们被要求求解率失真函数时，我们可以转而评估信息率失真函数。 定理4.1的证明将在接下来的两节中提供。 在本节的剩余部分，我们建立信息率失真函数 RI(D) 的一些有用性质，并提供一些简单的说明性例子来计算它。

--- 第 6 页 ---

**48**

**4 信源表示：率失真特性**

首先，由于约束 (4.9) 的左侧对于任何 PS^∣S 都满足

E[d(S,S^)]≥Dmin:=∑s∈SPS(s)mins^∈S^d(s,s^) , (4.11)

因此对于 D<Dmin，优化问题 (4.8) 是不可行的，我们可以在该区域简单地定义 RI(D)=∞。

其次，请注意 I(S;S^)=0 当且仅当 S 和 S^ 是独立的；见推论 3.3。 因此，如果存在一个概率质量函数 PS^ 使得

∑(s,s^)∈S×S^PS(s)PS^(s^)d(s,s^)≤D (4.12)

成立，我们可以在优化问题 (4.8) 中简单地令 PS^∣S=PS^，从而得到 RI(D)=0。由于总是成立

∑(s,s^)∈S×S^PS(s)PS^(s^)d(s,s^)≥Dmax:=mins^∈S^∑s∈SPS(s)d(s,s^) , (4.13)

对于 RI(D)=0，必须有 D≥Dmax。另一方面，这个条件也是充分的。 为了看到这一点，将达到 Dmax 的 s^ 的值记为 s^∗，并让 S^=s^∗ 的概率为1。 那么我们有 I(S;S^)=0，并且

E[d(S,S^)] =E[d(S,s^∗)] =∑s∈SPS(s)d(s,s^∗)=Dmax, (4.14)

对于任何 PS。 因此，对于任何 D≥Dmax，这种确定性选择 S^=s^∗ 是可行的，导致 RI(D)=0。总而言之，我们有以下关于 RI(D) 为零的充要条件。

**推论 4.1** 信息率失真函数 RI(D) 为零当且仅当

D≥Dmax=mins^∈S^∑s∈SPS(s)d(s,s^) 。 (4.15)

RI(D) 的一些进一步特征在下一个推论中给出。

**推论 4.2** 信息率失真函数 RI(D) 是一个关于 D 的非增凸函数。如果 RI(Dmin)>0，那么 RI(D) 在 D∈[Dmin,Dmax] 上是严格递减的，并且不等式约束 (4.9) 可以被一个等式约束替换。

**证明：** 考虑任何不小于 Dmin 的 D0<D1。将满足 E[d(S,S^)]≤D 的 PS^∣S 的集合记为 PD。 那么很明显 PD0⊆PD1，因此 R(D0)≥R(D1)。这证明了 RI(D) 是关于 D 的非增函数。

--- 第 7 页 ---

**4.2 香农信源编码基本定理 49**

对于凸性，我们需要证明对于任何不小于 Dmin 的 D0<D1 和任何 0≤λ≤1,

RI((1−λ)D0+λD1)≤(1−λ)RI(D0)+λRI(D1) (4.16)

成立。

用 PS^∣S(i),i=0,1 表示达到 RI(Di) 的条件概率分布。

定义

PS^∣S(λ)(s^∣s)=(1−λ)PS^∣S(0)(s^∣s)+λPS^∣S(1)(s^∣s) (4.17)

注意 PS^∣S(λ) 对于求解 RI((1−λ)D0+λD1) 的优化问题是可行的，因为在 PS^∣S(λ) 下

E[d(S,S^)]=(1−λ)∑(s,s^)∈S×S^PS(s)PS^∣S(0)(s^∣s)d(s,s^) +λ∑(s,s^)∈S×S^PS(s)PS^∣S(1)(s^∣s)d(s,s^) ≤(1−λ)D0+λD1 (4.18)

其中不等式来自假设 PS^∣S(0) 和 PS^∣S(1) 分别达到 RI(D0) 和 RI(D1)。 因此，根据约束优化问题公式 (4.8)-(4.9)，我们有

RI((1−λ)D0+λD1)≤I(λ)(S;S^) (4.19)

这是通过令条件概率分布 PS^∣S 为 PS^∣S(λ) 所达到的互信息。

另一方面，根据定理 3.7，I(S;S^) 在固定 PS 的情况下是关于 PS^∣S 的凸函数。 所以在 PS^∣S(λ) 下，我们有

I(λ)(S;S^)≤(1−λ)I(0)(S;S^)+λI(1)(S;S^) =(1−λ)RI(D0)+λRI(D1)。 (4.20)

将 (4.19) 和 (4.20) 结合起来，我们得到 (4.16) 并证明了 RI(D) 的凸性。

我们然后假设 RI(Dmin)>0 并证明 RI(D) 在 [Dmin,Dmax] 上是严格递减的。假设情况并非如此。 因为我们已经证明 RI(D) 是非增的，唯一的可能性是存在一些“平台”，即 Dmin≤D0<D1≤Dmax 且对于所有 D∈[D0,D1] 都有 RI(D)=RI(D0)。 见图 4.3 的示意图。因为我们已经证明 RI(D) 是

--- 第 8 页 ---

**50**

**4 信源表示：率失真特性**

凸的，通过重写

D1=Dmax−D0Dmax−D1D0+Dmax−D0D1−D0Dmax, (4.21)

我们有

RI(D1)≤Dmax−D0Dmax−D1RI(D0)+Dmax−D0D1−D0RI(Dmax). (4.22)

注意到根据我们的假设，RI(D1)=RI(D0) 成立，并且对于所有 D≥Dmax，RI(D)=0 成立，我们因此得出唯一的可能性是 RI(D0)=0，从而对于所有 D≥D0，RI(D)=0。但根据推论 4.1，对于 RI(D)=0，必须有 D≥Dmax。这因此导致了一个矛盾，所以 RI(D) 必须在 [Dmin,Dmax] 上是严格递减的。

*图 4.3：RI(D) 在 [Dmin,Dmax] 上严格递减的示意图。图中展示了一个函数R(D)，它在D0和D1之间有一个平台（水平线），这与函数的凸性相矛盾（虚线表示了凸性应有的走势）。*

**图 4.3**：RI(D) 在 [Dmin,Dmax] 上严格递减的示意图。

为了证明我们最后的论断，即不等式约束 (4.9) 可以被等式约束所取代，让我们假设存在某个 D∈[Dmin,Dmax]，使得在达到 RI(D) 时，相应的 PS^∣S 满足一个严格不等式 E[d(S,S^)]=D′<D。这便导致 RI(D′)=RI(D)，这与我们刚刚证明的 RI(D) 在 [Dmin,Dmax] 上的严格递减性质相矛盾。

**例 4.2** 在这个例子中，我们计算一个服从参数为 δ 的伯努利分布的信源 S 在汉明失真下的信息率失真函数。

让我们从寻找 Dmin 和 Dmax 开始。很明显 Dmin=0。根据定义 Dmax=mins^∈S^∑s∈SPS(s)d(s,s^), 我们有

Dmax=mins^∈{0,1}[(1−δ)d(0,s^)+δd(1,s^)]=min{δ,1−δ} 。 (4.23)

为确定起见，假设 δ≤1/2。因此 Dmax=δ 且对于所有 D≥δ，RI(D)=0。δ>1/2 的另一种情况可以类似处理，留给读者。

--- 第 9 页 ---

**4.2 香农信源编码基本定理 51**

现在让我们研究对于 D∈[0,δ] 的约束优化问题 (4.8)-(4.9)。为此，我们根据

I(S;S^)=H(S)−H(S∣S^), (4.24)

来展开 I(S;S^)，其中 H(S)=h2(δ) 是给定的，并且

H(S∣S^)=(a)H(S⊕S^∣S^) ≤H(S⊕S^) (4.25)

其中 (a) 是由于推论 3.6，因为在给定 S^ 的条件下，S 和 S⊕S^ 之间存在双射关系，而 (b) 是由于定理 3.6，“条件作用降低熵”。

但 S⊕S^ 是什么？实际上，{S⊕S^=1} 等价于 {S=S^}，在汉明失真下，这又等价于 {d(S,S^)=1}。 所以约束 (4.9)（由于推论 4.2，现在是一个等式约束）等价于 P(S⊕S^=1)=D。因此，我们得到了 I(S;S^) 的一个下界

I(S;S^)≥h2(δ)−h2(D) (4.26)

这对任何 PS^∣S 都成立。

如果我们能找到某个 PS^∣S 来达到下界 (4.26)，那么这个下界就恰好是 RI(D)。 检查到目前为止的推导步骤，我们看到所寻求的 PS^∣S 应该在 (4.25) 的 (b) 中达到等式； 也就是说，它应该使得 S^ 和 S⊕S^ 独立。 受到 S=S^⊕(S⊕S^) 这一事实的启发，我们将 S⊕S^ 视为一个与 S^ 无关的辅助随机变量 Z，根据上一段的讨论，它是一个参数为 D 的伯努利变量。 注意，这指定了“后向”条件概率分布 PS∣S^，而不是约束优化问题 (4.8)-(4.9) 中所需的“前向”条件概率分布 PS^∣S。 由于 PSPS^∣S=PS^PS∣S^, 为了指定 PS^∣S，我们还需要找到相应的 PS^。经过一些计算，不难验证这样的 PS^ 存在，并且由一个参数为 (δ−D)/(1−2D) 的伯努利分布给出。

因此，下界 (4.26) 是可达的，对于参数为 δ≤1/2 的伯努利信源和汉明失真，信息率失真函数由下式给出：

RI(D)=h2(δ)−h2(D) 如果 0≤D≤δ，否则为 0， (4.27)

如图 4.4 所示。

在例 4.2 中，首先指定 PS∣S^ 然后找到相应的 PS^ 来实现信息率失真函数的过程是一种有用的技术。 这样构造的 PS∣S^ 通常

--- 第 10 页 ---

**52**

**4 信源表示：率失真特性**

*图 4.4：带汉明失真的伯努利离散无记忆信源的率失真函数。图中显示了两条曲线，分别对应 δ=0.1 和 δ=0.5。两条曲线都是从 R(0) > 0 开始，随着 D 的增加而单调递减，直到 D=δ 时 R(D) 变为 0。*

**图4.4**：带汉明失真的伯努利DMS的率失真函数。

被称为“**测试信道**”。回想起来，因为我们是从 I(S;S^)=H(S)−H(S∣S^) 开始的，所以很自然地要考虑构造合适的 PS∣S^ 和 PS^，以便在满足失真约束和对 S 的概率分布的约束下，最大化 H(S∣S^)。

应该认识到，除了一些具有高度对称结构的极少数 PS 和 d（例如例 4.2 和本讲中的一些练习），测试信道的显式构造是困难的，我们不能期望信息率失真函数通常能得到闭合形式的解。 下一个例子通过仅轻微修改例 4.2 的设置来说明这一点。

**例 4.3** 我们考虑一个服从参数为 1/2 的伯努利分布的信源 S，具有以下非对称失真度量：

d(0,0)=d(1,1)=0, d(0,1)=1, 并且 d(1,0)=A>1 (4.28)

对于此设置，Dmin=0，Dmax=1/2。我们然后关注 D∈[0,1/2]。 根据推论 4.2，约束 (4.9) 是一个等式

E[d(S,S^)]=21PS^∣S(1∣0)+2APS^∣S(0∣1)=D. (4.29)

我们然后令 α=PS^∣S(1∣0)∈[0,1] 和 β=PS^∣S(0∣1)∈[0,1]。 利用它们，我们可以评估 I(S;S^) 得到

I(S;S^)=H(S)−H(S∣S^)=log2−[21−α+βh2(1−α+ββ)+21+α−βh2(1+α−βα)] (4.30)

这应该在 (α,β)∈[0,1]2 上，在 α+Aβ=2D 的约束下最小化。 不幸的是，这个优化问题似乎没有

--- 第 11 页 ---

**4.3 逆定理部分的证明 53**

一个封闭形式的解，只能通过数值方法来解决。 图 4.5 描绘了 A=1.5 和 3 时的 RI(D)。它还包括了 (4.27) 作为参考，对应于 A=1 的情况。

*图 4.5：具有非对称失真 (4.28) 的伯努利(1/2)离散无记忆信源的率失真函数。图中显示了A=1.0（虚线）、A=1.5（实线）和A=3.0（实线）三种情况下的R(D)曲线。所有曲线都从D=0时的R(D)=1开始，随D增加而下降，在D=0.5时达到0。A值越大，曲线在中间部分越高。*

**图 4.5**：具有非对称失真 (4.28) 的伯努利(1/2) DMS 的率失真函数。

尽管如此，存在用于计算信息率失真函数的有效数值算法，我们将在第9讲中介绍它们。

**4.3 逆定理部分的证明**

在本节中，我们建立定理4.1的逆定理部分，即 R(D)≥RI(D)。为此，我们将证明，对于任何信源编码器/译码器对，对于任何 R<RI(D)，都不可能达到不大于D的期望失真。

让我们固定一个任意的信源编码器和译码器对 (fn(s),gn(s))，信源消息长度为 n。 由于信源消息 S 是一个随机向量，(fn(s),gn(s)) 在 (S,S^) 上导出一个联合概率分布。 假设所产生的期望失真满足 E[d(S,S^)]≤D。我们现在考察这样一个信源编码器/译码器对的速率。 我们按以下步骤进行：

nR≥(a)H(W) ≥(b)I(S;W) ≥I(S;S^) =H(S)−H(S∣S^) =(c)∑i=1nH(Si)−∑i=1nH(Si∣S^,Si−1,...,S1) ≥(d)∑i=1nH(Si)−∑i=1nH(Si∣S^i) =∑i=1nI(Si;S^i), (4.31)

--- 第 12 页 ---

**54**

**4 信源表示：率失真特性**

其中，(a) 是因为根据 (4.1)，W 是 Mn 个索引之一，因此根据推论 3.1，其熵最多为 log2Mn≤nR 比特，根据定义 4.1，(b) 是由于定理 3.5，即 DPI，(c) 是由于定理 3.1，即熵的链式法则，而 (d) 是因为条件作用降低熵（见定理 3.6）。

为了进一步进行，我们引入一个辅助的“分时”随机变量 Q，它在 {1,2,...,n} 上均匀分布，并且与 S 无关。因此，我们可以继续前面的界定步骤：

R≥n1∑i=1nI(Si;S^i) =(e)I(SQ;S^Q∣Q) =H(SQ∣Q)−H(SQ∣S^Q,Q) (4.32)

其中 (e) 正是条件互信息的定义（见定义 2.15），注意到对于所有 1≤i≤n，PQ(i)=1/n。由于 DMS S 的概率分布不依赖于下标 Q，我们有 H(SQ∣Q)=H(SQ)。 另一方面，因为条件作用降低熵，H(SQ∣S^Q,Q)≤H(SQ∣S^Q)。所以我们得到

R≥H(SQ)−H(SQ∣S^Q)=I(SQ;S^Q). (4.33)

现在，注意到 SQ 的概率分布就是 PS，根据 RI(D) 的约束优化问题表述 (4.8)-(4.9)，我们可以更进一步得到

R≥I(SQ;S^Q) ≥RI(E[d(SQ,S^Q)])。 (4.34)

由于 Q 在 {1,2,...,n} 上是均匀的，使用全期望定律（见定理 2.3），我们有

E[d(SQ,S^Q)]=n1∑i=1nE[d(Si,S^i)] =(f)E[d(S,S^)]≤(g)D , (4.35)

其中 (f) 是由于失真的可加结构，即 (4.5)，而不等式 (g) 则源于证明开始时对 (fn(s),gn(s)) 的假设。 我们现在得出结论：

R≥RI(E[d(SQ,S^Q)]) ≥RI(D) (4.36)

--- 第 13 页 ---

**4.4 可达性部分的证明 55**

通过使用 RI(D) 的非增性质（见推论 4.2）。 这就完成了逆定理部分的证明。

**注 4.3** 检查证明过程，我们发现关于编码和译码的唯一要求是马尔可夫链 S↔W↔S^。我们不要求 W 在给定 S 的条件下是确定性的，也不要求 S^ 在给定 W 的条件下是确定性的。也就是说，即使信源编码器和译码器是随机的，逆定理仍然成立。

**4.4 可达性部分的证明**

在本节中，我们建立定理 4.1 的可达性部分，即 R(D)≤RI(D)。为此，我们将证明，给定 D，对于任何速率 R>RI(D)，存在一个信源编码器/译码器对的序列，以信源消息长度 n=1,2,... 为索引，使得 (R, D) 是一个可达的率失真对，其意义如定义 4.1 所述。

在开始之前，让我们把信源编码器/译码器对的概念具体化。 注意到代表 S 的索引 W 是从 Mn 个索引中选出的 fn(s)(S)，并且每个 W=w 对应一个再现的消息向量 gn(s)(w)。 我们称 {gn(s)(w),w=1,...,Mn} 为**码本**，并用 C 表示。C 的每一项称为一个**码字**，第 w 个码字，即 gn(s)(w)，也表示为 C(w)，其第 i 个元素进一步表示为 Ci(w)，对于 i=1,...,n。

编码器 fn(s) 指定了一个规则，将每个可能的信源消息向量分配给 C 中的一个码字，而译码器 gn(s) 只是输出这个分配的码字作为再现消息。 C 的维度参数 n 和 Mn 决定了速率 R 为 ⌈logMn⌉/n；见 (4.4)。 关于期望失真，我们有

E[d(S,S^)]=∑s∈SnPS(s)d(s,C(fn(s)(s))) 。 (4.37)

在这一点上，对于一个特定的 (fn(s),gn(s)) 对，通常很难对 E[d(S,S^)] 说些什么。 两个关键思想使得进一步的分析成为可能。首先，让码本本身是随机生成的； 其次，在分析期望失真时，让期望同时对信源消息和码本进行。

--- 第 14 页 ---

**56**

**4 信源表示：率失真特性**

**4.4.1 码本的生成**

我们固定某个满足 E[d(S,S^)]≤D 的 PS^∣S，并固定一个速率 R>I(S;S^)，该互信息是使用 PSPS^∣S 计算的。S^ 的边缘概率分布是 PS^(s^)=Σs∈SPS(s)PS^∣S(s^∣s)。 如前所述，我们让码本是随机生成的。具体来说，我们让所有码字的所有元素都是服从 PS^ 的独立同分布（i.i.d.）随机变量。 我们将这样一个随机码本表示为 C，以区别于它的实现 C。

请注意，这样一个随机码本 C 与信源消息 S 是独立的。一旦 C 生成，它就会被告知编码器和译码器。

将 S^n 视为一个空间，其中每个码字都是一个点。 C 的生成对应于根据 ∏i=1nPS^i 从 S^n 中独立有放回地抽样 Mn 次，其中 {S^i,i=1,...,n} 是服从 PS^ 的 i.i.d. 变量。

每个码字负责再现信源空间 Sn 中的一些点，而 Sn 中的每个点都需要由一个码字来再现。 因此，情况是 Sn 的空间应被划分为 Mn 个不重叠的子集，并且每个这样的子集都应与给定码本 C⊆S^n 中的一个不同码字相关联。 从直觉上可以清楚地看出，如果码本的大小 Mn 很小，并且 S^n 只是被稀疏地采样，那么每个码字就需要再现 Sn 的一个“大”子集中的点，从而导致大的失真。 因此，码本应包含足够多的码字，这一考虑转化为对最小速率 R 的要求。

**4.4.2 编码**

给定一个码本 C，我们应该如何对一个信源消息 s 进行编码？ 显然，最优的编码方式应该将 s 分配给能最小化 s 与码字之间失真的码字； 也就是说，选择

w∗=argminw=1,...,Mnd(s,C(w)), (4.38)

并令 s^=C(w∗)。 这本质上是一种“聚类”操作，它将 Sn 中的点围绕它们各自“最近”的码字进行聚类。 然而，最优编码过程的性能分析起来有些不便。 因此，我们转向另一种编码过程，它通常不是最优的，但仍然足以完成我们的可达性证明。 为了激发

--- 第 15 页 ---

**4.4 可达性部分的证明 57**

编码过程，让我们进行以下思想实验。 如果 S 和 C(W) 是根据 PS,S^=∏i=1nPSi,S^i 联合分布的，其中 {(Si,S^i),i=1,...,n} 是 i.i.d. 且服从 PS,S^，那么根据弱大数定律（见定理 2.4），当 n 无限增长时，以高概率 d(S,C(W)) 将会接近其期望 E[d(S,S^)]，这个期望在我们的证明开始时已经固定为不大于 D。 然而，现实并非如此，因为码本 C 是独立于 S 生成的。因此，我们希望编码器（在可能的情况下）选择某个码字，使得信源消息和所选码字的对表现得好像它们确实服从联合概率分布 PS,S^。

受前面讨论揭示的关键洞察的启发，我们设计了以下编码过程：在 ϵ>0 的条件下，给定 S=s，找到最小的索引 w∈{1,2,...,Mn} 使得

d(s,C(w))≤D+ϵ, (4.39) I(S;S^)−ϵ≤i(s;C(w))≤I(S;S^)+ϵ, (4.40)

其中

d(s,C(w))=n1∑i=1nd(si,Ci(w)), (4.41) i(s;C(w))=n1log2PS(s)PS^(C(w))PS,S^(s,C(w)) =n1∑i=1nlog2PS(si)PS^(Ci(w))PS,S^(si,Ci(w)). (4.42)

如果存在这样的索引 w，我们将其表示为 w* 并使用对应的码字 C(w∗) 来再现 s； 否则，编码过程失败，我们简单地设置 w∗=1 并使用 C(1) 来再现 S。

标准 (4.39) 很容易理解。 但我们为什么需要另一个标准 (4.40)？正如我们在思想实验中刚刚讨论的，我们希望编码器（在可能的情况下）选择某个码字，使得信源消息和所选码字的对表现得好像它们服从联合概率分布 PS,S。 鉴于弱大数定律，标准 (4.40) 有效地强制了这一点。 正如接下来将看到的，标准 (4.40) 促进了期望失真分析中的一个关键步骤。 为方便起见，对于给定的 C，我们为每个 s∈Sn 关联一个指示函数 E(C,s) 来指示是否发生编码失败； 也就是说，如果存在某个索引 w 同时满足 (4.39) 和 (4.40)，则 E(C,s)=0，否则 E(C,s)=1。

--- 第 16 页 ---

**58**

**4 信源表示：率失真特性**

**4.4.3 期望失真分析**

对于给定的码本 C，期望失真 E[d(S,S^)] 仍然难以评估。 如前所述，除了随机生成码本外，一个密切相关的关键思想是研究码本集合上的期望失真。 现在让我们考察期望失真，这个期望是对 S 和 C 两者同时取的：

EC,S[d(S,S^)]=∑C,s∈SnPC(C)PS(s)d(s,C(w)) =∑C,s∈Sn:E(C,s)=0PC(C)PS(s)d(s,C(w))

- ∑C,s∈Sn:E(C,s)=1PC(C)PS(s)d(s,C(1)). (4.43)

对于(4.43)中的第一个求和，由于每一项都满足(4.39)，所以 d(s,C(w))≤D+ϵ 成立，因此该求和也被 D+ϵ 上界。对于(4.43)中的第二个求和，无论如何我们都可以用 dmax 来上界 d(s,C(1))，因此该求和被 Pfdmax 上界，其中

Pf=∑C,s∈Sn:E(C,s)=1PC(C)PS(s) (4.44)

是编码失败的概率。

**4.4.4 编码失败概率的估计**

现在让我们估计可以重写为

Pf

Pf=∑C,s∈SnPC(C)PS(s)E(C,s) =∑s∈SnPS(s)∑CPC(C)E(C,s) (4.45)

一个关键步骤是 看到，对于任何 s∈Sn，

∑CPC(C)E(C,s)=[1−P((s,S^) 满足 (4.39)(4.40))]Mn。 (4.46)

这是因为 E(C,s)=1 意味着 C 的 Mn 个码字中没有一个同时满足 (4.39) 和 (4.40)，并且这些码字是 i.i.d. 的， 概率分布为 PS^。在这一点上，我们看到了对 C 取期望的好处：Pf 的评估现在只涉及 PS,S^，而不需要考虑任何特定的码本。

--- 第 17 页 ---

**4.4 可达性部分的证明 59**

为了进一步进行，让我们引入另一个指示函数 E~(s,s^) 如下：如果 (s,s^) 同时满足 (4.39) 和 (4.40)，则 E~(s,s^)=1，否则 E~(s,s^)=0。 所以

P((s,S^) 满足 (4.39)(4.40))=∑s^∈S^nPS^(s^)E~(s,s^) 。 (4.47)

现在标准 (4.40) 发挥作用，从中我们有 i(s;s^)≤I(S;S^)+ϵ, 这等价于

PS^(s^)≥2−n(I(S;S^)+ϵ)PS^∣S(s^∣s) 。 (4.48)

因此我们得到

P((s,S^) 满足 (4.39)(4.40)) =∑s^∈S^nPS^(s^)E~(s,s^) ≥2−n(I(S;S^)+ϵ)∑s^∈S^nPS^∣S(s^∣s)E~(s,s^). (4.49)

将 (4.49) 代回 (4.45)，我们有

Pf≤∑s∈SnPS(s)[1−2−n(I(S;S^)+ϵ)∑s^∈S^nPS^∣S(s^∣s)E~(s,s^)]Mn (4.50)

此时，我们应用不等式 (1−xy)n≤1−x+e−ny，对所有 0≤x,y≤1 成立，得到

Pf≤∑s∈SnPS(s)[1−∑s^∈S^nPS^∣S(s^∣s)E~(s,s^)+e−Mn2−n(I(S;S^)+ϵ)] =1−∑(s,s^)∈Sn×S^nPS,s^(s,s^)E~(s,s^)+e−Mn2−n(I(S;S^)+ϵ). (4.51)

在(4.51)中，第二项正是 E~(S,S^) 在 PS,S^ 下的期望，或者说是 (S,S^) 在 PS,S^ 下同时满足 (4.39) 和 (4.40) 的概率。 由于弱大数定律（见定理 2.4），对于任何 ϵ>0，当 n→∞ 时，这个概率趋近于 1。

在(4.51)中，第三项由 Mn2−n(I(S;S^)+ϵ) 随 n 无限增长时的渐近行为控制。 忽略 Mn 中可能存在的对于大 n 变得无关紧要的舍入效应，我们有

Mn2−n(I(S;S^)+ϵ)=2nR2−n(I(S;S^)+ϵ) =2n(R−I(S;S^)−ϵ) (4.52)

由于我们在证明开始时固定了 R>I(S;S^)，我们总可以

--- 第 18 页 ---

**60**

**4 信源表示：率失真特性**

找到一个足够小的 ϵ>0 使得 R−I(S;S^)−ϵ>0, 从而 (4.51) 中的第三项当 n→∞ 时收敛到零。

总而言之，通过考虑随机码本 C 的集合，我们已经证明了对于任何足够小的 ϵ>0, 当 n→∞ 时，编码失败的概率 Pf→0。

**4.4.5 总结步骤**

回到期望失真的分析，我们看到对于任何 ϵ>0, 当 n→∞ 时，EC,S[d(S,S^)]≤D+2ϵ 成立。 这意味着存在至少一个以 n 为索引的码本序列，对于这个序列，当 n→∞ 时，E[d(S,S^)]≤D+2ϵ 成立。† 让 ϵ→0，我们便可以断言，对于我们选择的 PS^∣S，任何 R>I(S;S^) 和 D 构成一个可达的率失真对。 通过选择能达到信息率失真函数的 PS^∣S，即 I(S;S^)=RI(D)，我们完成了可达性部分的证明。

**注 4.4** 回顾一下，唯一需要标准 (4.40) 的地方是对 Pf 的分析，从 (4.48) 开始。 如果编码过程忽略 (4.40)，相应的 Pf 将会更小。 但是包含 (4.40) 对于证明当 n→∞ 时 Pf→0 是至关重要的。

**注释**

在满足失真约束条件下表示信源的基本思想出现在香农1948年的论文[1, 第五部分]中，并在他1959年发表的著作[16]中得到系统发展，该著作奠定了量化和有损压缩的基础。 量化和有损压缩在信号（例如，图像、视频、音频、语音、生物医学、金融等）处理中得到了广泛应用。 在信息论教学中，将率失真理论放在如此靠前的位置，甚至在信道传输之前，有点不寻常，但我们这样做是考虑到从香农的一般通信系统模型中的信源部分开始似乎更自然（见第1讲图1.1）。 我们对香农信源编码基本定理可达性部分的证明是基于Robert J. McEliece的教科书[17]，并且没有明确依赖于许多其他教科书（例如，[18][8]）中的“标准”方法——典型性语言。 我们对典型性工具的介绍将推迟到第8讲。

一些教科书（例如，[8][9]）采用了一个不同的可达率失真对定义，通过用数学上更强的超额失真约束 limn→∞P(d(S,S^)≥D)=0 来替换期望失真约束 limn→∞E[d(S,S^)]≤D。

一本专门讨论率失真理论的经典参考文献是Toby Berger的[19]。 读者也被邀请阅读他与Jerry D. Gibson合著的更近期的综述[20]，其中提供了率失真理论发展（直到二十世纪末）的历史概览，包括俄罗斯信息论学派的贡献。 近年来，随着机器学习的快速发展，有损信源编码的新前沿已经出现并正在积极研究中，例如扩散模型等生成式编码技术（例如，[21]）和面向感知的有损压缩（例如，[22]）。

† 考虑一个日常生活中的例子，一个班级学生的平均身高是170cm，那么班级里必然至少有一个学生的身高不低于170cm。

--- 第 19 页 ---

**练习**

1. 考虑一个有两个分量 (S1,S2) 的信源，其中 S1 和 S2 是独立的，并让失真度量形式为 d((s1,s2),(s^1,s^2))=d1(s1,s^1)+d2(s2,s^2)。 将 Si 在失真 di 下的率失真函数表示为 Ri(D)，i=1,2。求率失真函数 R(D)，用 R1(D) 和 R2(D) 表示。

2. 如果一个DMS S在失真度量 d(s,s^),(s,s^)∈S×S^ 下的率失真函数是 R(D)，那么当失真度量变为 dk,b(s,s^)=kd(s,s^)+b 时，对于某些 k>0,b≥0，率失真函数是什么？

3. 我们可以用一个矩阵 D 来集体表示具有有限字母表的信源和再现的失真度量； 也就是说，D 的第 i 行第 j 列对应于 d(si,s^j)。 如果 D 满足其所有列都是某个向量 [d1,d2,...,d∣S∣] 的排列，证明率失真函数的以下下界：

   R(D)≥H(S)−H(V) (4.53)

   其中 V 是在所有满足

   ∑i=1∣S∣PV(i)di≤D.

   的 {1,2,...,∣S∣} 上的随机变量中熵最大的那个。

--- 第 20 页 ---

**62**

**4 信源表示：率失真特性**

此外，证明如果 S 是均匀的并且 D 的行是彼此的排列，那么下界 (4.53) 是紧的，即它恰好是率失真函数 R(D)。

1. 计算并绘制以下模型的率失真函数。 a) 信源字母表为 S={−1,0,1}，再现字母表为 S^={−1,1}，信源分布为均匀分布 PS(s)={1/3,1/3,1/3}，失真度量为

   D=![img](data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="0.875em" height="3.600em" viewBox="0 0 875 3600"><path d="M863,9c0,-2,-2,-5,-6,-9c0,0,-17,0,-17,0c-12.7,0,-19.3,0.3,-20,1 c-5.3,5.3,-10.3,11,-15,17c-242.7,294.7,-395.3,682,-458,1162c-21.3,163.3,-33.3,349, -36,557 l0,84c0.2,6,0,26,0,60c2,159.3,10,310.7,24,454c53.3,528,210, 949.7,470,1265c4.7,6,9.7,11.7,15,17c0.7,0.7,7,1,19,1c0,0,18,0,18,0c4,-4,6,-7,6,-9 c0,-2.7,-3.3,-8.7,-10,-18c-135.3,-192.7,-235.5,-414.3,-300.5,-665c-65,-250.7,-102.5, -544.7,-112.5,-882c-2,-104,-3,-167,-3,-189 l0,-92c0,-162.7,5.7,-314,17,-454c20.7,-272,63.7,-513,129,-723c65.3, -210,155.3,-396.3,270,-559c6.7,-9.3,10,-15.3,10,-18z"></path></svg>)001100![img](data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="0.875em" height="3.600em" viewBox="0 0 875 3600"><path d="M76,0c-16.7,0,-25,3,-25,9c0,2,2,6.3,6,13c21.3,28.7,42.3,60.3, 63,95c96.7,156.7,172.8,332.5,228.5,527.5c55.7,195,92.8,416.5,111.5,664.5 c11.3,139.3,17,290.7,17,454c0,28,1.7,43,3.3,45l0,9 c-3,4,-3.3,16.7,-3.3,38c0,162,-5.7,313.7,-17,455c-18.7,248,-55.8,469.3,-111.5,664 c-55.7,194.7,-131.8,370.3,-228.5,527c-20.7,34.7,-41.7,66.3,-63,95c-2,3.3,-4,7,-6,11 c0,7.3,5.7,11,17,11c0,0,11,0,11,0c9.3,0,14.3,-0.3,15,-1c5.3,-5.3,10.3,-11,15,-17 c242.7,-294.7,395.3,-681.7,458,-1161c21.3,-164.7,33.3,-350.7,36,-558 l0,-144c-2,-159.3,-10,-310.7,-24,-454c-53.3,-528,-210,-949.7, -470,-1265c-4.7,-6,-9.7,-11.7,-15,-17c-0.7,-0.7,-6.7,-1,-18,-1z"></path></svg>)

   b) 信源字母表为 S={−1,1}，再现字母表为 S^={−1,0,1}，信源分布为均匀分布 PS(s)={1/2,1/2}，失真度量为

   D=(011/21/210)

2. 如果一个DMS S是参数为 δ 的伯努利分布，并且失真度量是汉明失真，当 PS^∣S 达到 R(D) 时，描述编码过程 (4.38) 和 (4.39)-(4.40)，并找出它们的区别。

3. 考虑一个伯努利(1/2) DMS S。如果我们使用以下码本：

   C={0000,0101,1010,1111}, (4.54)

   用编码过程 (4.38) 计算期望汉明失真。

4. 假设有两个独立的伯努利 DMS S1 和 S2，参数分别为 δ1≤1/2 和 δ2≤1/2。 每次我们以无记忆的方式从其中一个 DMS 中采样，以得到一个新的 DMS，记为 S。设采样 S1 (或 S2) 的概率为 λ (或 1−λ)。 在汉明失真下，S 的率失真函数是什么？

5. 考虑一个在 S={1,...,m} 上均匀分布的 DMS S，再现字母表 S^=S，以及汉明失真度量。 计算率失真函数 R(D)。这也给出了一个费诺不等式（定理 3.9）达到等式的例子。

6. 在第 4.1 节的问题表述中，不假设失真度量 d 是从 S×S^ 到 [0,∞) 的映射，而是让它是一个根据某个条件概率分布 PU∣S,S^(u∣s,s^) 生成的随机变量 U∈[0,∞)。 推广定理 4.1 以解决这个扩展的问题

--- 第 21 页 ---

**4.4 可达性部分的证明 63**

表述，并指出在逆定理和可达性证明中必要的修改。

1. 我们对香农信源编码基本定理的陈述明确假设了失真度量 d(s,s^) 是有界的，即 dmax<∞。

   a) 解释为什么如果失真度量是无界的，即如果存在某个 (s,s^)∈S×S^ 使得 d(s,s^)=∞，定理的可达性部分的证明可能会失效。

   b) 考虑存在 s^∗∈S^ 使得对于所有 s∈S，d(s,s^∗)<∞ 的情况。论证在这种情况下，率失真函数 R(D) 仍然由定理 4.1 给出，即在 E[d(S,S^)]≤D 的约束下 minPS^∣SI(S;S^)。

   c) 论证存在某些 (S,S^,d(s,s^)) 的情况， 使得率失真函数必须高达 log∣S∣，即不可能有有效的表示，并且 R(D) 可能不同于 RI(D)。

   d) 计算以下设置的率失真函数：S={0,1}，S^={0,1,e}，S 是伯努利(1/2)，如果 s^=s 则 d(s,s^)=0，如果 s^=e 则为 1，如果 s^\\=s 且 s^\\=e 则为 ∞。